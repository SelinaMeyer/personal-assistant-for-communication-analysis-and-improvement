{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selinameyer/Documents/GitHub/personal-assistant-for-communication-analysis-and-improvement/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from openai import OpenAI\n",
    "from google.generativeai import caching\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "genai.configure(api_key=os.environ[\"API_KEY\"])\n",
    "openai_key = os.environ.get(\"OPENAI_KEY\")\n",
    "client = OpenAI(api_key=openai_key)\n",
    "gpt_3 = \"gpt-3.5-turbo-0125\"\n",
    "gpt_4 = \"gpt-4-0125-preview\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash',\n",
    "                             system_instruction=\"\"\"You are a communication expert tasked with judging the quality of advice given for different interactions between people. \n",
    "                              Reply `Yes`, if the quality of the advice is high and `No`if it is not. Only reply with `Yes`or `No`.\"\"\")\n",
    "good_advice = pd.read_csv(\"highly_rated_advice.csv\", index_col=\"Unnamed: 0\")\n",
    "bad_advice = pd.read_csv(\"low_rated_advice.csv\", index_col=\"Unnamed: 0\")\n",
    "\n",
    "orig_chat_file = pd.read_csv(\"chats.csv\")\n",
    "MAX_TOKENS = 1\n",
    "TEMP = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interaction_context_text(row):\n",
    "    few_shot_prompt = \"\"\n",
    "    if row[\"interaction_context\"] == \"Relationship\":\n",
    "        few_shot_prompt += \"Interaction Context: Whatsapp conversation between a romantic couple: \\n\"\n",
    "    elif row[\"interaction_context\"] == \"friendship\":\n",
    "        few_shot_prompt += \"Interaction Context: Whatsapp conversation between friends: \\n\"\n",
    "    elif row[\"interaction_context\"] == \"workplace-no-hierarchy\":\n",
    "        few_shot_prompt += \"Interaction Context: Email conversation between colleagues: \\n\"\n",
    "    elif row[\"interaction_context\"] == \"Workplace-hierarchy\":\n",
    "        few_shot_prompt += \"Interaction Context: Email conversation between employees and their boss: \\n\"\n",
    "    return few_shot_prompt\n",
    "\n",
    "def get_conversational_context_text(row):\n",
    "    few_shot_prompt = \"\"\n",
    "    if row[\"context\"] == \"Advice for victim\":\n",
    "        few_shot_prompt += f\"\"\"Conversation Context: Advice for {row[\"Victim\"]} who felt uneasy after the conversation\\n\"\"\"\n",
    "    elif row[\"context\"] == \"Advice for culprit\":\n",
    "        few_shot_prompt += f\"\"\"Conversation Context: Advice for {row[\"culprit\"]} who felt like they might have done something wrong after the conversation\\n\"\"\"\n",
    "    else:\n",
    "       few_shot_prompt +=  f\"\"\"Conversation Context: Advice for {row[\"Victim\"]} who didn't know how to react next.\\n\"\"\"\n",
    "    return few_shot_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"advice_quality_train_rating.csv\", index_col=\"Unnamed: 0\")\n",
    "test = pd.read_csv(\"advice_quality_test_rating.csv\", index_col=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = \"Judge if the advice given is of high quality given the interaction context and relationship between the speakrers. \\n\"\n",
    "for index, row in test.iterrows():\n",
    "   # few_shot_prompt = \"Judge if the advice given is of high quality given the interaction context and relationship between the speakrers. \\n\"\n",
    "    few_shot_prompt += get_interaction_context_text(row)\n",
    "    few_shot_prompt += f\"\"\"Chat: ```{row[\"chat_text\"]}```\\n\"\"\"\n",
    "    few_shot_prompt += get_conversational_context_text(row)\n",
    "    few_shot_prompt += f\"\"\"Advice: {row[\"advice_text\"]}\\n\"\"\"\n",
    "    few_shot_prompt += \"On a scale from 1 to 5, how good would you judge this advice? Provide an explanation for your rating first, then give your rating in a hash.\\n\"\n",
    "    few_shot_prompt += f\"Explanation: {row[\"text_feedback\"]}\\n Rating: {row[\"rating\"]}\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash',\n",
    "                             system_instruction=\"\"\"You are a communication expert tasked with judging the quality of advice given for different interactions between people.\n",
    "                             Explain your Rating first, then return your rating of the most recent advice on a scale of 1 (very bad) to 5 (very helpful)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "responses = {}\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    responses[index] = {}\n",
    "    responses[index][\"advice_id\"] = row[\"advice_id\"]\n",
    "    responses[index][\"rating\"] = row[\"rating\"]\n",
    "\n",
    "    this_few_shot_prompt = few_shot_prompt\n",
    "\n",
    "    #this_few_shot_prompt = \"Judge if the advice given is of high quality given the interaction context and relationship between the speakrers. \\n\"\n",
    "    this_few_shot_prompt += get_interaction_context_text(row)\n",
    "    this_few_shot_prompt += f\"\"\"Chat: ```{row[\"chat_text\"]}```\\n\"\"\"\n",
    "    this_few_shot_prompt += get_conversational_context_text(row)\n",
    "    this_few_shot_prompt += f\"\"\"Advice: {row[\"advice_text\"]}\\n\"\"\"\n",
    "    this_few_shot_prompt += \"On a scale from 1 to 5, how good would you judge this advice? Provide an explanation for your rating first, then give your rating in a hash.\\n\"\n",
    " \n",
    "    #response = model.generate_content(few_shot_prompt)\n",
    "    try:\n",
    "        response = model.generate_content([(few_shot_prompt)],\n",
    "                                          request_options={\"timeout\": 1000})\n",
    "        responses[index][\"model_output\"] = response.text\n",
    "    except Exception as e:\n",
    "        time.sleep(120)\n",
    "        response = model.generate_content([(few_shot_prompt)],\n",
    "                                          request_options={\"timeout\": 1000})\n",
    "        responses[index][\"model_output\"] = response.text\n",
    "    \n",
    "    time.sleep(4)\n",
    "    df = pd.DataFrame.from_dict(responses, orient='index')\n",
    "    df.to_csv(\"Gemini-advice-judgements_numerical_ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['model_rating'] = df['model_output'].str.extract(r'Rating:\\s+(\\d+)', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(model_rating\n",
       " 5    24\n",
       " 4     8\n",
       " 3     1\n",
       " Name: count, dtype: int64,\n",
       " rating\n",
       " 5    22\n",
       " 4     7\n",
       " 3     2\n",
       " 1     1\n",
       " 2     1\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.model_rating.value_counts(), df.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8484848484848485)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(df[\"rating\"], df[\"model_rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
