{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "from fireworks.client import Fireworks\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import anthropic \n",
    "import json\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('.env')\n",
    "\n",
    "# Get API keys from environment variables\n",
    "openai_key = os.environ.get(\"OPENAI_KEY\")\n",
    "anthropic_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "fireworks_key = os.environ.get(\"FIREWORKS_KEY\")\n",
    "\n",
    "# Initialize OpenAI, Anthropics, and Fireworks clients with respective API keys\n",
    "client = OpenAI(api_key=openai_key)\n",
    "anthropic_client = anthropic.Anthropic()\n",
    "fireworks_client = Fireworks(api_key=fireworks_key)\n",
    "\n",
    "# Define model names\n",
    "mixtral=\"accounts/fireworks/models/mixtral-8x22b-instruct\"\n",
    "llama3 =\"accounts/fireworks/models/llama-v3-70b-instruct\"\n",
    "\n",
    "# Define constants for token limit and temperature\n",
    "MAX_TOKENS = 500\n",
    "TEMP = 0.5\n",
    "\n",
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-4\",\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    temperature=TEMP,\n",
    "    stop=None,\n",
    "    seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Function to get completion from OpenAI's GPT-4 model.\n",
    "\n",
    "    Args:\n",
    "        messages (list[dict[str, str]]): List of message objects.\n",
    "        model (str, optional): Model name. Defaults to \"gpt-4\".\n",
    "        max_tokens (int, optional): Maximum number of tokens in the output. Defaults to MAX_TOKENS.\n",
    "        temperature (float, optional): Sampling temperature. Defaults to TEMP.\n",
    "        stop ([type], optional): Sequence of tokens to stop at. Defaults to None.\n",
    "        seed (int, optional): Random seed for deterministic output. Defaults to 123.\n",
    "        tools ([type], optional): Tools to use. Defaults to None.\n",
    "        logprobs ([type], optional): Whether to return log probabilities of the output tokens. Defaults to None.\n",
    "        top_logprobs ([type], optional): Number of top log probabilities to return. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        str: Completion from the model.\n",
    "    \"\"\"\n",
    "    # Prepare parameters for the API call\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop\": stop,\n",
    "        \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "\n",
    "    # Add tools to parameters if provided\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    # Call the API and get the completion\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion\n",
    "\n",
    "def get_fireworks_completion(messages, fireworks_model=mixtral):\n",
    "    \"\"\"\n",
    "    Function to get completion from Fireworks' model.\n",
    "\n",
    "    Args:\n",
    "        messages (list[dict[str, str]]): List of message objects.\n",
    "        fireworks_model (str, optional): Model name. Defaults to mixtral.\n",
    "\n",
    "    Returns:\n",
    "        [type]: Completion from the model.\n",
    "    \"\"\"\n",
    "    # Call the API and get the completion\n",
    "    chat_completion = fireworks_client.chat.completions.create(\n",
    "        model = fireworks_model,\n",
    "        messages = messages,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        temperature=TEMP,   \n",
    "    )\n",
    "    return chat_completion\n",
    "\n",
    "def get_anthropic_completion(messages):\n",
    "    \"\"\"\n",
    "    Function to get completion from Anthropics' model.\n",
    "\n",
    "    Args:\n",
    "        messages (list[dict[str, str]]): List of message objects.\n",
    "\n",
    "    Returns:\n",
    "        [type]: Completion from the model.\n",
    "    \"\"\"\n",
    "    # Call the API and get the completion\n",
    "    chat_completion = anthropic_client.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        temperature=TEMP,\n",
    "        messages=messages\n",
    "    )   \n",
    "    return chat_completion\n",
    "\n",
    "# Load data from CSV file\n",
    "df = pd.read_csv(\"Chat_selection.csv\")\n",
    "\n",
    "# Filter rows where Context is \"Relationship\"\n",
    "df_rel = df[df[\"Context\"] == \"Relationship\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_victim(entry):\n",
    "    \"\"\"\n",
    "    Function to generate a prompt for giving advice to the manipulated person based on the context of the conversation.\n",
    "\n",
    "    Args:\n",
    "        entry (dict): A dictionary containing details of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string that serves as the promot to generate advice for the manipulated person.\n",
    "    \"\"\"\n",
    "    # Check the context of the conversation and generate the appropriate prompt\n",
    "    if entry[\"Context\"] == \"Workplace-hierarchy\":\n",
    "        return f\"\"\"Below is an email conversation between me ({entry[\"Victim\"]}) and my boss ({entry[\"Perp\"]}):\n",
    "        {entry[\"Chat\"]}\n",
    "        The conversation leaves me uneasy, but I do not know why. What went wrong in this interaction?\"\"\"\n",
    "\n",
    "    elif entry[\"Context\"] == \"workplace-no-hierarchy\":\n",
    "        return f\"\"\"Below is an email conversation between me ({entry[\"Victim\"]}) and my colleagues:\n",
    "        {entry[\"Chat\"]}\n",
    "        The conversation leaves me uneasy, but I do not know why. What went wrong in this interaction?\"\"\"\n",
    "    \n",
    "    elif entry[\"Context\"] == \"friendship\":\n",
    "        return f\"\"\"Below is a Whatsapp conversation between me ({entry[\"Victim\"]}) and my friends:\n",
    "        {entry[\"Chat\"]}\n",
    "        The conversation leaves me uneasy, but I do not know why. What went wrong in this interaction?\"\"\"\n",
    "\n",
    "    elif entry[\"Context\"] == \"Relationship\":\n",
    "        return f\"\"\"Below is a Whatsapp conversation between me ({entry[\"Victim\"]}) and my romantic partner:\n",
    "        {entry[\"Chat\"]}\n",
    "        The conversation leaves me uneasy, but I do not know why. What went wrong in this interaction?\"\"\"\n",
    "\n",
    "def get_prompt_culprit(entry):\n",
    "    \"\"\"\n",
    "    Function to generate a prompt to give advice to the manipulating person based on the context of the conversation.\n",
    "\n",
    "    Args:\n",
    "        entry (dict): A dictionary containing details of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string that serves as the prompt for advice for the manipulating person.\n",
    "    \"\"\"\n",
    "    # Check the context of the conversation and generate the appropriate prompt\n",
    "    if entry[\"Context\"] == \"Workplace-hierarchy\":\n",
    "        return f\"\"\"Below is an email conversation between me ({entry[\"Perp\"]}) and my employees:\n",
    "        {entry[\"Chat\"]}\n",
    "        I feel I might have acted wrong in the conversation. Did I make any mistakes? What could I do better?\"\"\"\n",
    "\n",
    "    elif entry[\"Context\"] == \"workplace-no-hierarchy\":\n",
    "        return f\"\"\"Below is an email conversation between me ({entry[\"Perp\"]}) and my colleagues:\n",
    "        {entry[\"Chat\"]}\n",
    "        I feel I might have acted wrong in the conversation. Did I make any mistakes? What could I do better?\"\"\"\n",
    "    \n",
    "    elif entry[\"Context\"] == \"friendship\":\n",
    "        return f\"\"\"Below is a Whatsapp conversation between me ({entry[\"Perp\"]}) and my friends:\n",
    "        {entry[\"Chat\"]}\n",
    "         I feel I might have acted wrong in the conversation. Did I make any mistakes? What could I do better?\"\"\"\n",
    "    \n",
    "    elif entry[\"Context\"] == \"Relationship\":\n",
    "        return f\"\"\"Below is a Whatsapp conversation between me ({entry[\"Perp\"]}) and my romantic partner:\n",
    "        {entry[\"Chat\"]}\n",
    "         I feel I might have acted wrong in the conversation. Did I make any mistakes? What could I do better?\"\"\"\n",
    "        \n",
    "def get_prompt_what_next(entry):\n",
    "    \"\"\"\n",
    "    Function to generate a prompt for the next action based on the context of the conversation.\n",
    "\n",
    "    Args:\n",
    "        entry (dict): A dictionary containing details of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string that serves as the prompt for the next action.\n",
    "    \"\"\"\n",
    "    # Check the context of the conversation and generate the appropriate prompt\n",
    "    if entry[\"Context\"] == \"Workplace-hierarchy\":\n",
    "        return f\"\"\"Below is an email conversation between me ({entry[\"Victim\"]}) and my boss ({entry[\"Perp\"]}):\n",
    "        {entry[\"Chat\"]}\n",
    "        What should I say next? Why would that be the right reaction?\"\"\"\n",
    "\n",
    "    elif entry[\"Context\"] == \"workplace-no-hierarchy\":\n",
    "        return f\"\"\"Below is an email conversation between me ({entry[\"Victim\"]}) and my colleagues:\n",
    "        {entry[\"Chat\"]}\n",
    "         What should I say next? Why would that be the right reaction?\"\"\"\n",
    "    \n",
    "    elif entry[\"Context\"] == \"friendship\":\n",
    "        return f\"\"\"Below is a Whatsapp conversation between me ({entry[\"Victim\"]}) and my friends:\n",
    "        {entry[\"Chat\"]}\n",
    "         What should I say next? Why would that be the right reaction?\"\"\"\n",
    "\n",
    "    elif entry[\"Context\"] == \"Relationship\":\n",
    "        return f\"\"\"Below is a Whatsapp conversation between me ({entry[\"Victim\"]}) and my romantic partner:\n",
    "        {entry[\"Chat\"]}\n",
    "         What should I say next? Why would that be the right reaction?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = {}\n",
    "# Iterate over each row in the DataFrame\n",
    "for i, entry in df.iterrows():\n",
    "    responses[i] = {}\n",
    "\n",
    "    # Generate prompts for the victim, culprit, and what's next based on the entry\n",
    "    message_victim = [{\n",
    "        'role': 'user',\n",
    "        'content': get_prompt_victim(entry),\n",
    "      },\n",
    "    ]\n",
    "\n",
    "    message_culprit = [{\n",
    "        'role': 'user',\n",
    "        'content': get_prompt_culprit(entry),\n",
    "      },\n",
    "    ]\n",
    "\n",
    "    message_what_next = [{\n",
    "        'role': 'user',\n",
    "        'content': get_prompt_what_next(entry),\n",
    "      },\n",
    "    ]\n",
    "\n",
    "    # Get responses from different models for the victim\n",
    "    anthropic_response = get_anthropic_completion(messages=message_victim)\n",
    "    gpt_4_response = get_completion(messages=message_victim)\n",
    "    mixtral_response = get_fireworks_completion(messages=message_victim, fireworks_model=mixtral)\n",
    "    llama3_response = get_fireworks_completion(messages=message_victim, fireworks_model=llama3)\n",
    "\n",
    "    # Store the chat details and responses in the responses dictionary\n",
    "    responses[i][\"chat\"] = entry[\"Chat\"]\n",
    "    responses[i][\"context\"] = entry[\"Context\"]\n",
    "    responses[i][\"Victim\"] = entry[\"Victim\"]\n",
    "    responses[i][\"culprit\"] = entry[\"Perp\"]\n",
    "    responses[i][\"manipulation_strategy\"] = entry[\"Strategy\"]\n",
    "\n",
    "    responses[i][\"mixtral_instruct_response_victim\"] = mixtral_response.choices[0].message.content\n",
    "    responses[i][\"llama3_instruct_respons_victim\"] = llama3_response.choices[0].message.content\n",
    "    responses[i][\"gpt_4_response_victim\"] = gpt_4_response.choices[0].message.content\n",
    "    responses[i][\"claude_opus_response_victim\"] = anthropic_response.content[0].text\n",
    "\n",
    "    # Get responses from different models for the culprit\n",
    "    anthropic_response_culprit = get_anthropic_completion(messages=message_culprit)\n",
    "    gpt_4_response_culprit = get_completion(messages=message_culprit)\n",
    "    mixtral_response_culprit = get_fireworks_completion(messages=message_culprit, fireworks_model=mixtral)\n",
    "    llama3_response_culprit = get_fireworks_completion(messages=message_culprit, fireworks_model=llama3)\n",
    "\n",
    "    responses[i][\"gpt_4_response_culprit\"] = gpt_4_response_culprit.choices[0].message.content\n",
    "    responses[i][\"claude_opus_response_culprit\"] = anthropic_response_culprit.content[0].text\n",
    "    responses[i][\"mixtral_instruct_response_culprit\"] = mixtral_response_culprit.choices[0].message.content\n",
    "    responses[i][\"llama3_instruct_respons_culprit\"] = llama3_response_culprit.choices[0].message.content\n",
    "\n",
    "    # Get responses from different models for what's next\n",
    "    anthropic_response_what_next = get_anthropic_completion(messages=message_what_next)\n",
    "    gpt_4_response_what_next = get_completion(messages=message_what_next)\n",
    "    mixtral_response_what_next = get_fireworks_completion(messages=message_what_next, fireworks_model=mixtral)\n",
    "    llama3_response_what_next = get_fireworks_completion(messages=message_what_next, fireworks_model=llama3)\n",
    "\n",
    "    responses[i][\"gpt_4_response_what_next\"] = gpt_4_response_what_next.choices[0].message.content\n",
    "    responses[i][\"claude_opus_response_what_next\"] = anthropic_response_what_next.content[0].text\n",
    "    responses[i][\"mixtral_instruct_response_what_next\"] = mixtral_response_what_next.choices[0].message.content\n",
    "    responses[i][\"llama3_instruct_respons_what_next\"] = llama3_response_what_next.choices[0].message.content\n",
    "\n",
    "    # Convert the responses dictionary to a DataFrame and save it to a CSV file\n",
    "    df = pd.DataFrame.from_dict(responses, orient='index')\n",
    "    df.to_csv(\"generated_advice.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
